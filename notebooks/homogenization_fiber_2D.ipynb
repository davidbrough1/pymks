{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effective Stiffness of Fiber Composite\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This example demonstrates the use of the homogenization model from pyMKS on a set of fiber-like structures.  These structures are simulated to emulate fiber-reinforced polymer samples.  For a summary of homogenization theory and its use with effective stiffness properties please see the [Effective Siffness example](http://materialsinnovation.github.io/pymks/rst/stress_homogenization_2D.html).  This example will first generate a series of random microstructures with various fiber lengths and volume fraction.  The ability to vary the volume fraction is a new functionality of this example.  Then the generated stuctures will be used to calibrate and test the model based on simulated effective stress values.  Finally we will show that the simulated response compare favorably with those generated by the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Structures\n",
    "\n",
    "These first lines inport important packages that will be used to run pymks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are defining the parameters which we will use to create the microstructures.  `n_samples` will determine how many microstructures of a particular volume fraction we want to create.  `size` determines the number of pixels we want to be included in the microstructure.  We will define the material properties to be used in the finite element in `elastic_modulus`, `poissons_ratio` and `macro_strain`.  `n_phases` and `grain_size` will determine the physical characteristics of the microstructure.  We are using a high aspect ratio in creating our microstructures to simulate fiber-like structures.  The `volume_fraction` variable will be used to vary the fraction of each phase.  The sum of the volume fractions must be equal to 1.  The `percent_variance` variable introduces some variation in the volume fraction up to the specified percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_size = 200\n",
    "n_samples = 6 * [sample_size]\n",
    "size = (101, 101)\n",
    "elastic_modulus = (1.3, 75)\n",
    "poissons_ratio = (0.42, .22)\n",
    "macro_strain = 1.\n",
    "n_phases = 2\n",
    "grain_size = [(40, 2), (10, 2), (2, 40), (2, 10), (2, 30), (30, 2)]\n",
    "v_frac = [(0.8, 0.2), (0.7, 0.3), (0.6, 0.4), (0.5, 0.5), (0.3, 0.7), (0.4, 0.6)]\n",
    "per_ch = 0.1\n",
    "seed=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pymks.datasets.elastic_FE_simulation import ElasticFESimulation\n",
    "\n",
    "# sim = ElasticFESimulation(poissons_ratio=poissons_ratio,\n",
    "#                           elastic_modulus=elastic_modulus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create the microstructures and generate their responses using the `make_elastic_stress_random` function from pyMKS.  Four datasets are created to create the four different volume fractions that we are simulating.  Then the datasets are combined into one variable.   The volume fractions are listed in the variable `v_frac`.  Variation around the specified volume fraction can be obtained by varying `per_ch`.  The variation is randomly generated according a uniform distribution around the specified volume fraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 25, 25)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymks.datasets import make_microstructure\n",
    "X = make_microstructure(size=(25, 25),grain_size = (10,1),seed = seed,volume_fraction = (.80,.20))\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymks.tools import draw_microstructures\n",
    "draw_microstructures(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X = np.concatenate([make_microstructure(n_samples=sample, size=size,\n",
    "#                                         n_phases=2,\n",
    "#                                         grain_size=gs, seed=seed,\n",
    "#                                         volume_fraction=vf,\n",
    "#                                         percent_variance=per_ch)\n",
    "#                     for vf, gs, sample in zip(v_frac,\n",
    "#                                               grain_size, n_samples)])\n",
    "\n",
    "# print X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sim.run(X)\n",
    "# local_strain = sim.response\n",
    "# print local_strain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n",
    "Now we are going to print out a few microstructres to look at how the fiber length, orientation and volume fraction are varied.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from pymks import PrimitiveBasis\n",
    "\n",
    "# p_basis = PrimitiveBasis(2)\n",
    "\n",
    "# X_ = p_basis.discretize(X)\n",
    "# index = tuple([None for i in range(len(size) + 1)]) + (slice(None),)\n",
    "# modulus = np.sum(X_ * np.array(elastic_modulus)[index], axis=-1)\n",
    "# y_stress_field = local_strain * modulus\n",
    "# effective_stress = np.average(y_stress_field.reshape(len(y_stress_field), -1), axis=1)\n",
    "# print effective_stress.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "# pickle.dump((X, effective_stress), open('fiber_data.pkl', 'wb'))\n",
    "dataset, stresses = pickle.load(open('fiber_data.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.average(stresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moduli = stresses / macro_strain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymks.tools import draw_microstructures\n",
    "examples = dataset[::sample_size]\n",
    "draw_microstructures(examples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Point Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymks.stats import correlate\n",
    "from pymks import PrimitiveBasis\n",
    "\n",
    "\n",
    "p_basis = PrimitiveBasis(n_states=2, domain=[0, 1])\n",
    "data_stats = correlate(dataset[::sample_size], p_basis, correlations=[(0, 0)], periodic_axes=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymks.tools import draw_correlations\n",
    "\n",
    "print data_stats.shape\n",
    "draw_correlations(np.transpose(data_stats, (3, 1, 2, 0))[0], correlations=[('fiber', 'fiber')] * 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "\n",
    "Next we are going to initiate the model. The MKSHomogenizationModel takes in microstructures and runs two-point statistics on them to get a statistical representation of the microstructures.  An expalnation of the use of two-point statistics can be found in the [Checkerboard Microstructure Example](http://materialsinnovation.github.io/pymks/rst/checker_board.html).  Then the model uses PCA and regression models to create a linkage between the calcualted properties and structures.  \n",
    "Here we simply initiate the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymks import MKSHomogenizationModel\n",
    "\n",
    "model = MKSHomogenizationModel(basis=p_basis, correlations=[(0, 0), (1, 1)], periodic_axes=[0, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to split our data into testing and training segments so we can test and see if our model can accurately predict the effective stress. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "flat_shape = (dataset.shape[0],) + (dataset[0].size,)\n",
    "data_train, data_test, moduli_train, moduli_test = train_test_split(\n",
    "    dataset.reshape(flat_shape), moduli, test_size=0.2, random_state=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use sklearn's GridSearchCV to optimize the `n_components` and `degree` for our model. Let's search over the range of 1st order to 3rd order polynomial for `degree` and 2 to 7 principal components for `n_components`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "mse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "params_to_tune = {'degree': np.arange(1, 7), 'n_components': np.arange(2, 8)}\n",
    "fit_params = {'size': dataset[0].shape}\n",
    "gs = GridSearchCV(model, params_to_tune, fit_params=fit_params, scoring=mse_scorer).fit(data_train, moduli_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Order of Polynomial'), (gs.best_estimator_.degree)\n",
    "print('Number of Components'), (gs.best_estimator_.n_components)\n",
    "print('Score'), (gs.score(data_test, moduli_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymks.tools import draw_gridscores_matrix\n",
    "\n",
    "draw_gridscores_matrix(gs, ['n_components', 'degree'], score_label='MSE',\n",
    "                       param_labels=['Number of Components', 'Order of Polynomial'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymks.tools import draw_gridscores\n",
    "\n",
    "# gs_deg_1 = [x for x in gs.grid_scores_ \\\n",
    "#             if x.parameters['degree'] == 1]\n",
    "_n = 6\n",
    "gs_deg_2 = [x for x in gs.grid_scores_ \\\n",
    "            if x.parameters['degree'] == 2][:_n]\n",
    "gs_deg_3 = [x for x in gs.grid_scores_ \\\n",
    "            if x.parameters['degree'] == 3][:_n]\n",
    "gs_deg_4 = [x for x in gs.grid_scores_ \\\n",
    "            if x.parameters['degree'] == 4][:_n]\n",
    "# gs_deg_5 = [x for x in gs.grid_scores_ \\\n",
    "#             if x.parameters['degree'] == 5][:_n]\n",
    "# gs_deg_6 = [x for x in gs.grid_scores_ \\\n",
    "#             if x.parameters['degree'] == 6][:_n]\n",
    "\n",
    "draw_gridscores([gs_deg_2, gs_deg_3, gs_deg_4], 'n_components', \n",
    "                data_labels=['2nd Order', '3rd Order', '4th Order'],\n",
    "                param_label='Number of Components', score_label='MSE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best model was found to have `degree` equal to 3 and `n_components` equal to 5. Let's go ahead and use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model = gs.best_estimator_\n",
    "model.n_components = 4\n",
    "model.degree = 3\n",
    "model.fit(data_train.reshape((data_train.shape[0],) + dataset.shape[1:]), moduli_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structures in PCA space\n",
    "\n",
    "Now we want to draw how the samples are spread out in PCA space and look at how the testing and training data line up.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pymks.tools import draw_component_variance\n",
    "\n",
    "draw_component_variance(model.dimension_reducer.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymks.tools import draw_components\n",
    "\n",
    "# print 2 * 101 * 101\n",
    "# print model.dimension_reducer.components_.reshape((7, 202 * 101))\n",
    "\n",
    "# draw_components(model.dimension_reducer.components_.reshape((7, 202 * 101))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymks.tools import draw_components_scatter\n",
    "\n",
    "\n",
    "moduli_predict = model.predict(data_test.reshape((data_test.shape[0],) + dataset.shape[1:]))\n",
    "draw_components_scatter([model.reduced_fit_data[:, :3],\n",
    "                         model.reduced_predict_data[:, :3]],\n",
    "                        ['Training Data', 'Testing Data'],\n",
    "                        legend_outside=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there is pretty good agreement between the testing and the training data.  We can also see that the four different fiber sizes are seperated in the PC space.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percent Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Predicted Percent Error 1.79726777343\n",
      "Mean Training Percent Error 1.7217184613\n"
     ]
    }
   ],
   "source": [
    "_moduli_train = model.predict(data_train.reshape((data_train.shape[0],) + dataset.shape[1:]))\n",
    "pred_percent_error = 100 * np.abs(moduli_predict - moduli_test) / moduli_test\n",
    "train_percent_error = 100 * np.abs(_moduli_train - moduli_train) / moduli_train\n",
    "pred_absolute_error = np.abs(moduli_predict - moduli_test)\n",
    "# train_absolute_error = np.abs(_moduli_train - moduli_train)\n",
    "print 'Mean Predicted Percent Error', np.mean(pred_percent_error)\n",
    "print 'Mean Training Percent Error', np.mean(train_percent_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(stresses.flatten(), train_absolute_error.flatten(), 'o', color='#1a9850', label='Training Dataset')\n",
    "plt.plot(stress_test.flatten(), pred_absolute_error.flatten(), 'o', color='#f46d43', label='Testing Dataset')\n",
    "plt.xlabel('Effective Stress (MPa)', fontsize=15)\n",
    "plt.ylabel('Abosolute Error (MPa)', fontsize=15)\n",
    "plt.legend(fontsize=12, loc=2)\n",
    "plt.title('Absolute Error', fontsize=20)\n",
    "plt.ylim(-0.05, np.max(train_absolute_error) + 0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.hist(stresses.flatten())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Yay! There is a good corrolation between the FE results and those predicted by our linkage.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.120282325262\n"
     ]
    }
   ],
   "source": [
    "# Finding the volume fractions of generated microstructures\n",
    "#d1 = sum(sum(dataset[0,:,:]))/(101.0*101.0)\n",
    "#print d1\n",
    "Vf = np.zeros(dataset.shape[0])\n",
    "#print Vf.shape\n",
    "for i in range(0,dataset.shape[0]): Vf[i] = sum(sum(dataset[i,:,:]))/(101.0*101.0)\n",
    "macro_strain = 1.0\n",
    "\n",
    "print Vf[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vf = volume fraction of the fibers\n",
    "# Ef = Elastic modulus of the fibers\n",
    "# Em = Elastic modulus of the matrix\n",
    "\n",
    "Ef = 75\n",
    "Em = 1.3\n",
    "\n",
    "E_Manera = Vf*((16/45)*Ef+2*Em) + 8/9*Em \n",
    "Manera_moduli = np.concatenate((moduli[None], E_Manera[None]))\n",
    "# Stress_Manera = E_Manera*macro_strain?\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# l = average fiber length\n",
    "# d = average fiber diameter\n",
    "# grain_size = [(40, 2), (10, 2), (2, 40), (2, 10), (2, 30), (30, 2)]\n",
    "d = np.zeros(dataset.shape[0])\n",
    "l = np.zeros(dataset.shape[0])\n",
    "l[0:200] = 40\n",
    "d[0:200] = 2\n",
    "l[200:400] = 10\n",
    "d[200:400] = 2\n",
    "l[400:600] = 2\n",
    "d[400:600] = 40\n",
    "l[600:800] = 2\n",
    "d[600:800] = 10\n",
    "l[800:1000] = 2\n",
    "d[800:1000] = 30\n",
    "l[1000:1200] = 30\n",
    "d[1000:1200] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nu_l = ((Ef/Em)-1)/((Ef/Em)+(2*l/d))\n",
    "# Logitudinal elastic modulus\n",
    "El = Em*((1+(2*l/d)*nu_l*Vf)/(1-nu_l*Vf))\n",
    "nu_t = ((Ef/Em)-1)/((Ef/Em)+(2))\n",
    "# Traverse elastic Modulus\n",
    "Et = Em*((1+(2)*nu_t*Vf)/(1-nu_t*Vf))\n",
    "#Elastic modulus of composite with randomly oriented fibers \n",
    "Er = (3/8)*(El) + (5/8)*(Et)\n",
    "Er.shape\n",
    "\n",
    "Halpin_Tsai_moduli = np.concatenate((moduli[None], El[None]))\n",
    "# Stresses_Halpin_Tsai = Er*macro_strain\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Rule of mixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "E_mix_u = (1-Vf)*Em+(Vf)*Ef\n",
    "Mixtures_upper_moduli = np.concatenate((moduli[None], E_mix_u[None]))\n",
    "E_mix_l = (Ef*Em)/(((1-Vf)*Ef)+(Vf*Em))\n",
    "Mixtures_lower_moduli = np.concatenate((moduli[None], E_mix_l[None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw Goodness of fit\n",
    "\n",
    "Now we are going to look at how well our model predicts the properties of the structures.  The calculated properties will be plotted against the properties generated by the model.  We should see a linear realtionship with a slope of 1.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymks.tools import _draw_goodness_of_fit\n",
    "\n",
    "_moduli_train = model.predict(data_train.reshape((data_train.shape[0],) + dataset.shape[1:]))\n",
    "\n",
    "fit_data = np.array([moduli_train, _moduli_train])\n",
    "pred_data = np.array([moduli_test, moduli_predict])\n",
    "_draw_goodness_of_fit(fit_data, pred_data, Mixtures_upper_moduli, Halpin_Tsai_moduli,\n",
    "                      ['Training Data', 'Testing Data', 'ROM', 'Halpin-Tsai'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
